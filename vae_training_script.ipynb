{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script for training VAE with NASNet.\n",
        "# Made by [oblivisheee](https://github.com/oblivisheee) and [Integrio Team](https://github.com/Integrio-Team).\n",
        "*Recommended to copy in your drive and customise for yourself.*<br>\n",
        "*Its alpha version, for now, im fixing all problems.*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jChtPT46BVbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NzHjyWCmDCn"
      },
      "outputs": [],
      "source": [
        "# @title # Installing requirements.\n",
        "connect_google_drive = True # @param {type:\"boolean\"}\n",
        "if connect_google_drive == True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "\n",
        "\n",
        "!pip install safetensors\n",
        "!pip install matplotlib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda, BatchNormalization, Reshape, Conv2DTranspose, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import NASNetLarge\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from safetensors.tensorflow import save_file\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Installing NASNet.\n",
        "amount_of_filters = 3 # @param {type:\"integer\"}\n",
        "width_of_shape = 600 # @param {type:\"integer\"}\n",
        "height_of_shape = 850 # @param {type:\"integer\"}\n",
        "# Входное изображение\n",
        "input_shape = (height_of_shape, width_of_shape, amount_of_filters)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "include_top = False # @param {type:\"boolean\"}\n",
        "# Загрузка предварительно обученной модели NASNetLarge\n",
        "weights_type = 'imagenet' # @param [\"imagenet\", \"custom\"]\n",
        "if weights_type == 'custom':\n",
        "  file_path_weights = None\n",
        "  weights_type = file_path_weights\n",
        "  # @markdown * Enter file path of weights if you chose \"custom\".\n",
        "  file_path_weights = \"\" # @param {type:\"string\"}\n",
        "nasnet_model = NASNetLarge(weights=weights_type, include_top=include_top, input_tensor=input_tensor)"
      ],
      "metadata": {
        "id": "oKINxu57t-JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWI0WzbqmGA7"
      },
      "outputs": [],
      "source": [
        "# @title # Model config and compile.\n",
        "\n",
        "latent_amount = 256 # @param {type:\"integer\"}\n",
        "# @markdown * Size of filters like Conv2D and Dense.\n",
        "size_of_filters = 128 # @param {type:\"integer\"}\n",
        "\n",
        "factors_of_reshape = None\n",
        "\n",
        "\n",
        "for factor in range(5, size_of_filters + 1):\n",
        "    if size_of_filters % factor == 0:\n",
        "        second_factor = size_of_filters // factor\n",
        "        break\n",
        "# Encoder\n",
        "x = Conv2D(size_of_filters, (3, 3), activation='relu', padding='same')(nasnet_model.output)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(size_of_filters, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(size_of_filters, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "# @markdown * Paremeter of against overtrain.\n",
        "dropout_rate = 0.15 # @param {type:\"number\"}\n",
        "x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
        "z_mean = Dense(latent_amount)(x)\n",
        "z_log_var = Dense(latent_amount)(x)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_amount), mean=0.0, stddev=1.0)\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_amount,))([z_mean, z_log_var])\n",
        "\n",
        "class CustomLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        result = self.custom_add(inputs)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_add(inputs):\n",
        "        return tf.add(inputs, 1)\n",
        "\n",
        "custom_layer = CustomLayer()\n",
        "z_mean_custom = custom_layer(z_mean)\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "# @markdown * Weight of Regularization Layer\n",
        "\n",
        "weight_of_regularization_layer = 0.01 # @param {type:\"number\"}\n",
        "class CustomRegularizationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, weight=weight_of_regularization_layer, **kwargs):\n",
        "        super(CustomRegularizationLayer, self).__init__(**kwargs)\n",
        "        self.weight = weight\n",
        "\n",
        "    def call(self, inputs):\n",
        "        regularization_loss = tf.reduce_sum(tf.square(inputs)) * self.weight\n",
        "        self.add_loss(regularization_loss, inputs=inputs)\n",
        "        return inputs\n",
        "\n",
        "regularization_layer = CustomRegularizationLayer(weight=0.01)\n",
        "\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "regularized_inputs = regularization_layer(input_layer)\n",
        "\n",
        "x = Dense(size_of_filters, activation='relu')(regularized_inputs)\n",
        "output_layer = Dense(size_of_filters, activation='softmax')(x)\n",
        "x = Dense(size_of_filters, activation='relu', kernel_initializer='glorot_uniform')(input_tensor)\n",
        "x = Conv2D(size_of_filters, (3, 3), activation='relu', padding='same')(nasnet_model.output)\n",
        "\n",
        "dynamic_of_sampling_layer = True # @param {type:\"boolean\"}\n",
        "class SamplingLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), mean=0.0, stddev=1.0)\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0]\n",
        "\n",
        "sampling_layer = SamplingLayer(dynamic=dynamic_of_sampling_layer)\n",
        "\n",
        "#Saving encoder\n",
        "encoder = Model(input_tensor, [z_mean, z_log_var, z_mean_custom])\n",
        "encoder_output = encoder(input_tensor)\n",
        "\n",
        "# Decoder\n",
        "decoder_input = Input(shape=(latent_amount,))\n",
        "sampled_z = Lambda(sampling, output_shape=(latent_amount,))([z_mean, z_log_var])\n",
        "x = Dense(size_of_filters, activation='relu')(decoder_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Reshape((factor, second_factor, 1))(x)\n",
        "x = Conv2DTranspose(size_of_filters, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2DTranspose(size_of_filters, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoder_output = Conv2DTranspose(amount_of_filters, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "# Saving decoder\n",
        "decoder = Model(decoder_input, decoder_output)\n",
        "\n",
        "sampled_z = sampling_layer([z_mean, z_log_var])\n",
        "vae_input = Input(shape=input_shape, name='vae_input')\n",
        "z_mean, z_log_var, z_mean_custom = encoder(vae_input)\n",
        "z = Lambda(sampling, output_shape=(latent_amount,))([z_mean, z_log_var])\n",
        "vae_output = decoder(z)\n",
        "\n",
        "\n",
        "output_layer_resized = tf.image.resize(vae_output, size=(height_of_shape, width_of_shape), method='nearest')\n",
        "\n",
        "# vae_loss\n",
        "reconstruction_loss = tf.reduce_mean(tf.square(vae_input - output_layer_resized), axis=(1, 2, 3))\n",
        "kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "vae_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "vae = Model(vae_input, output_layer_resized)\n",
        "\n",
        "vae.add_loss(vae_loss)\n",
        "# Model compiling.\n",
        "vae.compile(optimizer='Adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfKGRdzonADj"
      },
      "outputs": [],
      "source": [
        "# @title # Set path of train data.\n",
        "import os\n",
        "main_dir = \"/content/datasets/\" # @param {type:\"string\"}\n",
        "train_data_dir = \"/content/datasets/train_data\" # @param {type:\"string\"}\n",
        "validation_data_dir = \"/content/datasets/validation_dir/\" # @param {type:\"string\"}\n",
        "if not os.path.exists(main_dir):\n",
        "    os.makedirs(main_dir)\n",
        "if not os.path.exists(train_data_dir):\n",
        "    os.makedirs(train_data_dir)\n",
        "if not os.path.exists(validation_data_dir):\n",
        "    os.makedirs(validation_data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzefW48CmW1y",
        "outputId": "2780fb19-0d3d-41c1-c5b6-1aad7f750f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 557 images belonging to 3 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ]
        }
      ],
      "source": [
        "# @title # Dataset config.\n",
        "def preprocess_image(image):\n",
        "    if image.shape[-1] == 3:\n",
        "        image = tf.image.central_crop(image, central_fraction=0.8)\n",
        "        image = tf.image.resize(image, size=(height_of_shape, width_of_shape))\n",
        "    else:\n",
        "        image = tf.image.rgb_to_grayscale(image)\n",
        "        image = tf.image.central_crop(image, central_fraction=0.8)\n",
        "        image = tf.image.resize(image, size=(height_of_shape, width_of_shape))\n",
        "    return image\n",
        "\n",
        "rescale = 255 # @param {type:\"integer\"}\n",
        "rotation_range = 20 # @param {type:\"integer\"}\n",
        "width_shift_range = 0.2 # @param {type:\"number\"}\n",
        "height_shift_range = 0.2 # @param {type:\"number\"}\n",
        "shear_range = 0.2 # @param {type:\"number\"}\n",
        "zoom_range = 0.3 # @param {type:\"number\"}\n",
        "horizontal_flip = True # @param {type:\"boolean\"}\n",
        "fill_mode = \"nearest\" # @param [\"nearest\", \"reflect\", \"wrap\", \"constant\"]\n",
        "\n",
        "class_mode = \"input\" # @param [\"input\", \"categorical\", \"binary\", \"sparse\"]\n",
        "shuffle = True # @param {type:\"boolean\"}\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/rescale,\n",
        "    rotation_range=rotation_range,\n",
        "    width_shift_range=width_shift_range,\n",
        "    height_shift_range=height_shift_range,\n",
        "    shear_range=shear_range,\n",
        "    zoom_range=zoom_range,\n",
        "    horizontal_flip=horizontal_flip,\n",
        "    fill_mode=fill_mode\n",
        ")\n",
        "\n",
        "batch_size = 5 # @param {type:\"integer\"}\n",
        "image_size = (height_of_shape, width_of_shape)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=class_mode,\n",
        "    color_mode='rgb',\n",
        "    shuffle=shuffle\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=class_mode,\n",
        "    color_mode='rgb',\n",
        "    shuffle=shuffle\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: train_generator,\n",
        "    output_signature=tf.TensorSpec(shape=(None, *image_size, 3), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: validation_generator,\n",
        "    output_signature=tf.TensorSpec(shape=(None, *image_size, 3), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_image)\n",
        "validation_dataset = validation_dataset.map(preprocess_image)\n",
        "\n",
        "num_epochs_for_repeat = 15 # @param {type:\"integer\"}\n",
        "train_dataset = train_dataset.repeat(num_epochs_for_repeat)\n",
        "validation_dataset = validation_dataset.repeat(num_epochs_for_repeat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBe3UCYsmXyP"
      },
      "outputs": [],
      "source": [
        "# @title # Start training.\n",
        "\n",
        "num_epochs = 1 # @param {type:\"integer\"}\n",
        "learning_rate = 0.01 # @param {type:\"number\"}\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "checkpoint_per_epoch = True # @param {type:\"boolean\"}\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='vae_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "\n",
        "main_dir = \"/content/trained-models/\" # @param {type:\"string\"}\n",
        "models_dir = \"/content/trained-models/main-models/\" # @param {type:\"string\"}\n",
        "checkpoint_models_dir = \"/content/trained-models/saved_models_per_epoch/\" # @param {type:\"string\"}\n",
        "if not os.path.exists(main_dir):\n",
        "    os.makedirs(main_dir)\n",
        "if not os.path.exists(checkpoint_models_dir) and checkpoint_per_epoch == True:\n",
        "    os.makedirs(checkpoint_models_dir)\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "\n",
        "name_of_model = \"!change_ME!\" # @param {type:\"string\"}\n",
        "safetensors_ext_name = \".safetensors\"\n",
        "\n",
        "class SaveSafetensors(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        name_of_model = \"manga-diffusion-vae_epoch_{:02d}\".format(epoch)\n",
        "        tensors = {\"embedding\": tf.zeros((height_of_shape, width_of_shape)), \"attention\": tf.zeros((height_of_shape, width_of_shape))}\n",
        "        save_file(tensors, os.path.join(checkpoint_models_dir, name_of_model + safetensors_ext_name))\n",
        "        print(\"\\nEpoch-checkpoint saved in \" + checkpoint_models_dir + \" with the name: \" + name_of_model + \".safetensors\")\n",
        "\n",
        "\n",
        "save_safetensors_callback = SaveSafetensors()\n",
        "\n",
        "\n",
        "callbacks = [reduce_lr]\n",
        "\n",
        "\n",
        "if checkpoint_per_epoch:\n",
        "    callbacks.append(save_safetensors_callback)\n",
        "\n",
        "\n",
        "history = vae.fit(\n",
        "    train_generator,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    validation_steps=len(validation_generator),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "\n",
        "os.chdir(models_dir)\n",
        "tensors = {\"embedding\": tf.zeros((height_of_shape, width_of_shape)), \"attention\": tf.zeros((height_of_shape, width_of_shape))}\n",
        "save_file(tensors, name_of_model + safetensors_ext_name)\n",
        "print(\"Model with name \" + name_of_model + safetensors_ext_name + \" saved in \" + models_dir + \" .\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hJaGS-yihTp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title #Test the model.\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, Lambda, Layer, Conv2DTranspose, Reshape, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "color_mode = \"grayscale\" # @param [\"rgb\", \"grayscale\"]\n",
        "\n",
        "def preprocess_image(image_path, target_size):\n",
        "    img = image.load_img(image_path, target_size=target_size, color_mode=color_mode)\n",
        "    img = image.img_to_array(img)\n",
        "\n",
        "\n",
        "    if color_mode == \"grayscale\":\n",
        "      img = tf.image.grayscale_to_rgb(tf.convert_to_tensor(img, dtype=tf.float32))\n",
        "\n",
        "    img = img / rescale\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "\n",
        "def visualize_images(original_img, generated_img):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title('Original Image')\n",
        "    plt.imshow(original_img[0, :, :, 0], cmap='gray')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title('Generated Image')\n",
        "    plt.imshow(generated_img[0, :, :, 0], cmap='gray')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "model_link = \"/content/trained-models/main-models/\" + name_of_model + safetensors_ext_name\n",
        "\n",
        "image_path = '' # @param {type:\"string\"}\n",
        "target_size = (height_of_shape, width_of_shape, 3)\n",
        "num_steps = 30 # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "input_image = preprocess_image(image_path, target_size)\n",
        "generated_image = vae.predict(input_image, steps=num_steps)\n",
        "visualize_images(input_image, generated_image)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}